{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b78a98b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a858889d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -1.017336650686711\n"
     ]
    }
   ],
   "source": [
    "#generating the data\n",
    "D=2\n",
    "def create_data(N, K):\n",
    "    X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
    "    y = np.zeros(N*K, dtype='uint8') # class labels\n",
    "    for j in range(K):\n",
    "        ix = range(N*j,N*(j+1))\n",
    "        r = np.linspace(0.0,1,N) # radius\n",
    "        t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        y[ix] = j\n",
    "    return X, y\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self,n_inputs , n_neurons):\n",
    "        self.weights = np.random.randn(n_inputs,n_neurons)# both arguments are the shape \n",
    "        self.biases = np.zeros((1, n_neurons))# 1 is the argument for shape\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights)+self.biases\n",
    "\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0,inputs)\n",
    "\n",
    "class Activation_softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs -np.max(inputs, axis= 1 , keepdims =True))\n",
    "        probailities = exp_values /np.sum(exp_values, axis=1 , keepdims = True)  \n",
    "        self.output = probailities \n",
    "\n",
    "class Loss:\n",
    "    def calculate(self , output , y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "                \n",
    "class loss_CategoricalCrossentropy(Loss):\n",
    "     def forward(Self , y_pred , y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7 , 1-1e-7)\n",
    "                            \n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) ==2:\n",
    "              correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
    "         \n",
    "        negative_log_likelihoods = np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "               \n",
    "                            \n",
    "\n",
    "X, y = create_data(100 ,3)        \n",
    "dense1 = Layer_Dense(2,3)\n",
    "activation1 = Activation_ReLU()\n",
    "                            \n",
    "dense2 = Layer_Dense(3,3)\n",
    "activation2 = Activation_softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "            \n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "                            \n",
    "\n",
    "loss_function = loss_CategoricalCrossentropy()\n",
    "loss = loss_function.calculate(activation2.output , y)\n",
    "print(\"Loss:\", loss)\n",
    "                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875a9e24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
